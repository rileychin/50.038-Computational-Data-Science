{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDS_py_ry_hy_direct_video_classifier_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpKNXKabHwf7"
      },
      "source": [
        " # https://keras.io/examples/vision/video_classification/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HweLHQIHJi1"
      },
      "source": [
        "### Importing and preparing dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqPhsNJyQ46",
        "outputId": "6d3b438f-ce9a-41f9-b5f8-d9a1907fbeab"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4zrBUpM3zPL"
      },
      "source": [
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W29darr8tVu",
        "outputId": "f86bd6c0-99fd-44e7-dce5-f1a2cf48b036"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFd2nRiX-Pim"
      },
      "source": [
        "datapath = '/content/gdrive/MyDrive/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xZnYEiEx-8Z"
      },
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MhdKMvyzid"
      },
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqdykdjvzIZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f3f419-ffe5-4f6b-a59e-ffd4a42c392a"
      },
      "source": [
        "# open the file which have names of training videos\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "\n",
        "badvideos = [f for f in listdir(f\"{datapath}badtrain\") if isfile(join(f\"{datapath}badtrain\",f))]\n",
        "goodvideos = [f for f in listdir(f\"{datapath}goodtrain\") if isfile(join(f\"{datapath}goodtrain\",f))]\n",
        "allvideos = badvideos+goodvideos\n",
        "\n",
        "videoclass = ['bad' for _ in range(len(badvideos))] + ['good' for _ in range(len(goodvideos))]\n",
        "\n",
        "# creating a dataframe having video names\n",
        "train = pd.DataFrame({'video_name':allvideos})\n",
        "train['class'] = videoclass\n",
        "\n",
        "print(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             video_name class\n",
            "0   20211024_221837.mp4   bad\n",
            "1   20211024_221843.mp4   bad\n",
            "2   20211024_221849.mp4   bad\n",
            "3   20211024_221854.mp4   bad\n",
            "4   20211024_221859.mp4   bad\n",
            "..                  ...   ...\n",
            "68        ry_good_4.MOV  good\n",
            "69        ry_good_2.MOV  good\n",
            "70        ry_good_5.MOV  good\n",
            "71        ry_good_6.MOV  good\n",
            "72       ry_good_10.MOV  good\n",
            "\n",
            "[73 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLbJi1t7zyCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680c8235-3ee0-4113-976b-4093883179b5"
      },
      "source": [
        "# open the .txt file which have names of test videos\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "\n",
        "badtest = [f for f in listdir(f\"{datapath}badtest\") if isfile(join(f\"{datapath}badtest\",f))]\n",
        "goodtest = [f for f in listdir(f\"{datapath}goodtest\") if isfile(join(f\"{datapath}goodtest\",f))]\n",
        "alltest = badtest+goodtest\n",
        "\n",
        "testclass = ['bad' for _ in range(len(badtest))] + ['good' for _ in range(len(goodtest))]\n",
        "\n",
        "# creating a dataframe having video names\n",
        "test = pd.DataFrame({'video_name':alltest})\n",
        "test['class'] = testclass\n",
        "\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     video_name class\n",
            "0     Bad 2.mov   bad\n",
            "1     Bad 4.mov   bad\n",
            "2     Bad 3.mov   bad\n",
            "3     Bad 6.mov   bad\n",
            "4     Bad 5.mov   bad\n",
            "5     Bad 7.mov   bad\n",
            "6    Bad 10.mov   bad\n",
            "7     Bad 9.mov   bad\n",
            "8     Bad 8.mov   bad\n",
            "9    Bad 12.mov   bad\n",
            "10   Bad 11.mov   bad\n",
            "11    Bad 1.mov   bad\n",
            "12   Bad 13.mov   bad\n",
            "13   Bad 14.mov   bad\n",
            "14   Bad 15.mov   bad\n",
            "15   Good 2.mov  good\n",
            "16   Good 3.mp3  good\n",
            "17   Good 6.mov  good\n",
            "18   Good 7.mov  good\n",
            "19   Good 1.mov  good\n",
            "20   Good 8.mov  good\n",
            "21   Good 9.mov  good\n",
            "22   Good 4.mov  good\n",
            "23  Good 10.mov  good\n",
            "24  Good 12.mov  good\n",
            "25  Good 11.mov  good\n",
            "26  Good 13.mov  good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsDGz1MEG4jA"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usHh8hWc0Rml"
      },
      "source": [
        "def load_video(path, i, max_frames=0):\n",
        "    if path == \"train\":\n",
        "        videoname = train['video_name'][i]\n",
        "        videoclass = train['class'][i]\n",
        "        video = datapath+videoclass+\"train/\"+videoname\n",
        "    elif path == \"test\":\n",
        "        videoname = test['video_name'][i]\n",
        "        videoclass = test['class'][i]\n",
        "        video = datapath+videoclass+\"test/\"+videoname\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frameID = cap.get(1)\n",
        "            if (frameID%2==0):\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "                frames.append(frame)\n",
        "            # storing the frames in a new folder named train_1\n",
        "            # filename = f\"{path}_1/{train['video_name'][i]}_frame{frameID/2}_{train['class'][i]}.jpg\"\n",
        "            # cv2.imwrite(filename, frame)\n",
        "            # if len(frames) == max_frames:\n",
        "            #     break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV2uWmkk1sUp"
      },
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX8q_AsV2xMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dee4c7-9f8a-4f50-8377-111248e6ae7b"
      },
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train['class'])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bad', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEMYf5rK3QDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86e57fd-5498-4335-8ab0-e1ebf2104b65"
      },
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"class\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        # print(os.path.join(datapath, \"bad\"+ root_dir, path))\n",
        "        frames = load_video(root_dir, idx)\n",
        "        # frames2 = load_video(os.path.join(datapath, \"bad\"+root_dir, path), idx)\n",
        "        frames = frames[None, ...]\n",
        "        # print(frames)\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train, \"train\")\n",
        "# test_data, test_labels = prepare_all_videos(test, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (73, 20, 2048)\n",
            "Frame masks in train set: (73, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G38AYHXLCvEZ",
        "outputId": "d398f691-bb4d-430a-afc5-1451fcf4d61c"
      },
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(\n",
        "        frame_features_input, mask=mask_input\n",
        "    )\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 8s - loss: 0.7961 - accuracy: 0.3750\n",
            "Epoch 00001: val_loss improved from inf to 1.24986, saving model to /tmp/video_classifier\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.7224 - accuracy: 0.5490 - val_loss: 1.2499 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4684 - accuracy: 0.8438\n",
            "Epoch 00002: val_loss did not improve from 1.24986\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.5535 - accuracy: 0.7059 - val_loss: 1.2585 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4691 - accuracy: 0.7812\n",
            "Epoch 00003: val_loss improved from 1.24986 to 1.22207, saving model to /tmp/video_classifier\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 0.5285 - accuracy: 0.7059 - val_loss: 1.2221 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5790 - accuracy: 0.7812\n",
            "Epoch 00004: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.6027 - accuracy: 0.7255 - val_loss: 1.2603 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5275 - accuracy: 0.7188\n",
            "Epoch 00005: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.5636 - accuracy: 0.6863 - val_loss: 1.3805 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6172 - accuracy: 0.6875\n",
            "Epoch 00006: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.6125 - accuracy: 0.7059 - val_loss: 1.4265 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4695 - accuracy: 0.8125\n",
            "Epoch 00007: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.5010 - accuracy: 0.7451 - val_loss: 1.4697 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5498 - accuracy: 0.7188\n",
            "Epoch 00008: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.5591 - accuracy: 0.7255 - val_loss: 1.5148 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5820 - accuracy: 0.6250\n",
            "Epoch 00009: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.5551 - accuracy: 0.6667 - val_loss: 1.5585 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5388 - accuracy: 0.7188\n",
            "Epoch 00010: val_loss did not improve from 1.22207\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.5455 - accuracy: 0.7059 - val_loss: 1.5943 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6924 - accuracy: 0.5556\n",
            "Test accuracy: 55.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRtOZsM7Gkg7"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQKmKOkxPSXs",
        "outputId": "1e87b809-d2fe-4e46-91f9-76f4f7503296"
      },
      "source": [
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(i):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(\"test\", i)\n",
        "    frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "for idx, video in enumerate(test[\"video_name\"].values.tolist()):\n",
        "    print(f\"Test video path: {video}\")\n",
        "    test_frames = sequence_prediction(idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test video path: Bad 2.mov\n",
            "  bad: 62.45%\n",
            "  good: 37.55%\n",
            "Test video path: Bad 4.mov\n",
            "  bad: 68.52%\n",
            "  good: 31.48%\n",
            "Test video path: Bad 3.mov\n",
            "  bad: 67.41%\n",
            "  good: 32.59%\n",
            "Test video path: Bad 6.mov\n",
            "  bad: 67.43%\n",
            "  good: 32.57%\n",
            "Test video path: Bad 5.mov\n",
            "  bad: 66.45%\n",
            "  good: 33.55%\n",
            "Test video path: Bad 7.mov\n",
            "  bad: 70.39%\n",
            "  good: 29.61%\n",
            "Test video path: Bad 10.mov\n",
            "  bad: 68.35%\n",
            "  good: 31.65%\n",
            "Test video path: Bad 9.mov\n",
            "  bad: 67.48%\n",
            "  good: 32.52%\n",
            "Test video path: Bad 8.mov\n",
            "  bad: 67.88%\n",
            "  good: 32.12%\n",
            "Test video path: Bad 12.mov\n",
            "  bad: 67.91%\n",
            "  good: 32.09%\n",
            "Test video path: Bad 11.mov\n",
            "  bad: 63.73%\n",
            "  good: 36.27%\n",
            "Test video path: Bad 1.mov\n",
            "  bad: 65.55%\n",
            "  good: 34.45%\n",
            "Test video path: Bad 13.mov\n",
            "  bad: 69.36%\n",
            "  good: 30.64%\n",
            "Test video path: Bad 14.mov\n",
            "  bad: 65.76%\n",
            "  good: 34.24%\n",
            "Test video path: Bad 15.mov\n",
            "  bad: 64.59%\n",
            "  good: 35.41%\n",
            "Test video path: Good 2.mov\n",
            "  bad: 67.10%\n",
            "  good: 32.90%\n",
            "Test video path: Good 3.mp3\n",
            "  bad: 67.80%\n",
            "  good: 32.20%\n",
            "Test video path: Good 6.mov\n",
            "  bad: 64.13%\n",
            "  good: 35.87%\n",
            "Test video path: Good 7.mov\n",
            "  bad: 67.15%\n",
            "  good: 32.85%\n",
            "Test video path: Good 1.mov\n",
            "  bad: 66.83%\n",
            "  good: 33.17%\n",
            "Test video path: Good 8.mov\n",
            "  bad: 63.83%\n",
            "  good: 36.17%\n",
            "Test video path: Good 9.mov\n",
            "  bad: 64.22%\n",
            "  good: 35.78%\n",
            "Test video path: Good 4.mov\n",
            "  bad: 65.44%\n",
            "  good: 34.56%\n",
            "Test video path: Good 10.mov\n",
            "  bad: 66.12%\n",
            "  good: 33.88%\n",
            "Test video path: Good 12.mov\n",
            "  bad: 66.01%\n",
            "  good: 33.99%\n",
            "Test video path: Good 11.mov\n",
            "  bad: 64.35%\n",
            "  good: 35.65%\n",
            "Test video path: Good 13.mov\n",
            "  bad: 67.88%\n",
            "  good: 32.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_put52pvE9zI"
      },
      "source": [
        "# def sequence_prediction(path, i):\n",
        "#     class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "#     # frames = load_video(os.path.join(\"test\", path), i)\n",
        "#     test_data, test_labels = prepare_all_videos(test, \"test\")\n",
        "#     probabilities = sequence_model.predict(test_data)[0]\n",
        "\n",
        "#     for i in np.argsort(probabilities)[::-1]:\n",
        "#         print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "#     # return frames\n",
        "\n",
        "# for idx, video in enumerate(test[\"video_name\"].values.tolist()):\n",
        "#     print(f\"Test video path: {video}\")\n",
        "#     test_frames = sequence_prediction(video, idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}